{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ad4adf-41dd-437f-a84e-d33ddf2a4d3d",
   "metadata": {},
   "source": [
    "- Name: Deep Salunkhe\r\n",
    "- Roll No.:21102A0014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b408579e-f8df-492b-b659-24382445973f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users:\n",
      "1. User 1: [0.11036971 0.60075994 0.16833709 0.00914947 0.43507662 0.28023306\n",
      " 0.02290225 0.79640019 0.98295488 0.26142673]\n",
      "2. User 2: [0.425977   0.65766369 0.9701737  0.65879481 0.60786215 0.07416477\n",
      " 0.95750273 0.06190925 0.34324352 0.81100159]\n",
      "3. User 3: [0.88062676 0.64899705 0.49912342 0.20537359 0.18012605 0.65777237\n",
      " 0.27083193 0.45187614 0.14996548 0.17305661]\n",
      "\n",
      "Companies:\n",
      "1. Company 1: [0.15329739 0.8407762  0.91807368 0.42883621 0.15573593 0.06765897\n",
      " 0.22817119 0.65363924 0.30933041 0.21559711]\n",
      "2. Company 2: [0.03646343 0.85845221 0.16556934 0.88705666 0.50235906 0.96790686\n",
      " 0.55354814 0.35085778 0.38667873 0.16502561]\n",
      "3. Company 3: [0.17942199 0.00126556 0.27302754 0.25692292 0.57520424 0.63306435\n",
      " 0.01608568 0.08660631 0.86455036 0.62893808]\n",
      "4. Company 4: [0.12834332 0.96050937 0.07815112 0.38392233 0.41483286 0.19376785\n",
      " 0.33531162 0.91722339 0.06874432 0.34610868]\n",
      "5. Company 5: [0.21907787 0.69588818 0.24097526 0.45871519 0.7790164  0.67787636\n",
      " 0.92702882 0.9476927  0.21480999 0.53158108]\n",
      "6. Company 6: [0.61120607 0.75131388 0.0405824  0.02794155 0.12865411 0.58715622\n",
      " 0.71908204 0.05193806 0.90686114 0.87958723]\n",
      "7. Company 7: [0.16163406 0.11340534 0.39113475 0.67713112 0.12810157 0.00231448\n",
      " 0.96871683 0.19354144 0.09138731 0.79290645]\n",
      "8. Company 8: [0.79942391 0.86844686 0.85474881 0.02766085 0.91467487 0.11048766\n",
      " 0.19830592 0.93453914 0.98203221 0.215408  ]\n",
      "9. Company 9: [0.00544703 0.69451812 0.42939063 0.11546643 0.10420998 0.55269172\n",
      " 0.36279096 0.84073908 0.02686234 0.99649595]\n",
      "10. Company 10: [0.13408179 0.21567755 0.00670276 0.08388703 0.75317958 0.87495841\n",
      " 0.31811653 0.06462547 0.23948954 0.31523869]\n",
      "\n",
      "Cosine Similarity Recommendations:\n",
      "User 1: ['Company 6', 'Company 5', 'Company 8']\n",
      "  Features: [array([0.61120607, 0.75131388, 0.0405824 , 0.02794155, 0.12865411,\n",
      "       0.58715622, 0.71908204, 0.05193806, 0.90686114, 0.87958723]), array([0.21907787, 0.69588818, 0.24097526, 0.45871519, 0.7790164 ,\n",
      "       0.67787636, 0.92702882, 0.9476927 , 0.21480999, 0.53158108]), array([0.79942391, 0.86844686, 0.85474881, 0.02766085, 0.91467487,\n",
      "       0.11048766, 0.19830592, 0.93453914, 0.98203221, 0.215408  ])]\n",
      "User 2: ['Company 7', 'Company 5', 'Company 8']\n",
      "  Features: [array([0.16163406, 0.11340534, 0.39113475, 0.67713112, 0.12810157,\n",
      "       0.00231448, 0.96871683, 0.19354144, 0.09138731, 0.79290645]), array([0.21907787, 0.69588818, 0.24097526, 0.45871519, 0.7790164 ,\n",
      "       0.67787636, 0.92702882, 0.9476927 , 0.21480999, 0.53158108]), array([0.79942391, 0.86844686, 0.85474881, 0.02766085, 0.91467487,\n",
      "       0.11048766, 0.19830592, 0.93453914, 0.98203221, 0.215408  ])]\n",
      "User 3: ['Company 2', 'Company 5', 'Company 8']\n",
      "  Features: [array([0.03646343, 0.85845221, 0.16556934, 0.88705666, 0.50235906,\n",
      "       0.96790686, 0.55354814, 0.35085778, 0.38667873, 0.16502561]), array([0.21907787, 0.69588818, 0.24097526, 0.45871519, 0.7790164 ,\n",
      "       0.67787636, 0.92702882, 0.9476927 , 0.21480999, 0.53158108]), array([0.79942391, 0.86844686, 0.85474881, 0.02766085, 0.91467487,\n",
      "       0.11048766, 0.19830592, 0.93453914, 0.98203221, 0.215408  ])]\n",
      "\n",
      "TF-IDF Recommendations:\n",
      "User 1: ['Company 6', 'Company 5', 'Company 8']\n",
      "  Features: [array([0.61120607, 0.75131388, 0.0405824 , 0.02794155, 0.12865411,\n",
      "       0.58715622, 0.71908204, 0.05193806, 0.90686114, 0.87958723]), array([0.21907787, 0.69588818, 0.24097526, 0.45871519, 0.7790164 ,\n",
      "       0.67787636, 0.92702882, 0.9476927 , 0.21480999, 0.53158108]), array([0.79942391, 0.86844686, 0.85474881, 0.02766085, 0.91467487,\n",
      "       0.11048766, 0.19830592, 0.93453914, 0.98203221, 0.215408  ])]\n",
      "User 2: ['Company 7', 'Company 5', 'Company 8']\n",
      "  Features: [array([0.16163406, 0.11340534, 0.39113475, 0.67713112, 0.12810157,\n",
      "       0.00231448, 0.96871683, 0.19354144, 0.09138731, 0.79290645]), array([0.21907787, 0.69588818, 0.24097526, 0.45871519, 0.7790164 ,\n",
      "       0.67787636, 0.92702882, 0.9476927 , 0.21480999, 0.53158108]), array([0.79942391, 0.86844686, 0.85474881, 0.02766085, 0.91467487,\n",
      "       0.11048766, 0.19830592, 0.93453914, 0.98203221, 0.215408  ])]\n",
      "User 3: ['Company 2', 'Company 5', 'Company 8']\n",
      "  Features: [array([0.03646343, 0.85845221, 0.16556934, 0.88705666, 0.50235906,\n",
      "       0.96790686, 0.55354814, 0.35085778, 0.38667873, 0.16502561]), array([0.21907787, 0.69588818, 0.24097526, 0.45871519, 0.7790164 ,\n",
      "       0.67787636, 0.92702882, 0.9476927 , 0.21480999, 0.53158108]), array([0.79942391, 0.86844686, 0.85474881, 0.02766085, 0.91467487,\n",
      "       0.11048766, 0.19830592, 0.93453914, 0.98203221, 0.215408  ])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def generate_random_data(num_users, num_companies, num_features):\n",
    "    \"\"\"Generates random preference vectors and company features with names.\"\"\"\n",
    "    user_preferences = np.random.rand(num_users, num_features)\n",
    "    company_features = np.random.rand(num_companies, num_features)\n",
    "\n",
    "    # Generate random names for users and companies\n",
    "    user_names = [f\"User {i+1}\" for i in range(num_users)]\n",
    "    company_names = [f\"Company {i+1}\" for i in range(num_companies)]\n",
    "\n",
    "    return user_preferences, company_features, user_names, company_names\n",
    "\n",
    "def cosine_similarity_recommendation(user_preferences, company_features):\n",
    "    \"\"\"Recommends jobs using cosine similarity.\"\"\"\n",
    "    similarities = user_preferences.dot(company_features.T)\n",
    "    top_3_indices = np.argsort(similarities, axis=1)[:, -3:]  # Get top 3 indices per user\n",
    "    return top_3_indices\n",
    "\n",
    "def tfidf_recommendation(user_preferences_text, company_features_text):\n",
    "    \"\"\"Recommends jobs using TF-IDF.\"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    user_vectors = vectorizer.fit_transform(user_preferences_text)\n",
    "    company_vectors = vectorizer.transform(company_features_text)\n",
    "\n",
    "    # Handle potential one-dimensional user_vectors:\n",
    "    if user_vectors.shape[1] == 1:\n",
    "        user_vectors = user_vectors.reshape(-1, 1)\n",
    "\n",
    "    similarities = user_vectors.dot(company_vectors.T)\n",
    "    top_3_indices = np.argsort(similarities, axis=1)[:, -3:]  # Get top 3 indices per user\n",
    "    return top_3_indices\n",
    "\n",
    "# Example usage with flexibility for numerical or textual data\n",
    "num_users = 3\n",
    "num_companies = 10\n",
    "num_features = 10\n",
    "\n",
    "user_preferences, company_features, user_names, company_names = generate_random_data(num_users, num_companies, num_features)\n",
    "\n",
    "# **Optional** Convert to text format for TF-IDF (consider data type)\n",
    "# user_preferences_text = [\" \".join(map(str, user_pref)) for user_pref in user_preferences]\n",
    "# company_features_text = [\" \".join(map(str, company_feat)) for company_feat in company_features]\n",
    "\n",
    "# **Choose appropriate recommendation based on data type**\n",
    "if isinstance(user_preferences[0][0], str):  # If data is textual\n",
    "    cosine_recommendations = cosine_similarity_recommendation(user_preferences_text, company_features_text)\n",
    "    tfidf_recommendations = tfidf_recommendation(user_preferences_text, company_features_text)\n",
    "else:  # If data is numerical\n",
    "    cosine_recommendations = cosine_similarity_recommendation(user_preferences, company_features)\n",
    "    tfidf_recommendations = cosine_similarity_recommendation(user_preferences, company_features)  # Same for numerical data\n",
    "\n",
    "print(\"Users:\")\n",
    "for i, user in enumerate(user_names):\n",
    "    print(f\"{i+1}. {user}: {user_preferences[i]}\")\n",
    "\n",
    "print(\"\\nCompanies:\")\n",
    "for i, company in enumerate(company_names):\n",
    "    print(f\"{i+1}. {company}: {company_features[i]}\")\n",
    "\n",
    "print(\"\\nCosine Similarity Recommendations:\")\n",
    "for i, user_id in enumerate(range(num_users)):\n",
    "    print(f\"User {user_id+1}:\", [company_names[idx] for idx in cosine_recommendations[i]])\n",
    "    print(\"  Features:\", [company_features[idx] for idx in cosine_recommendations[i]])\n",
    "\n",
    "print(\"\\nTF-IDF Recommendations:\")\n",
    "for i, user_id in enumerate(range(num_users)):\n",
    "    print(f\"User {user_id+1}:\", [company_names[idx] for idx in tfidf_recommendations[i]])\n",
    "    print(\"  Features:\", [company_features[idx] for idx in tfidf_recommendations[i]])\n",
    "\n",
    "# Example of different results\n",
    "user_preferences[0][0] = 0.9  # Adjust user preferences\n",
    "user_preferences[0][1] = 0.1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07b893d5-b35b-4bd0-8a94-84fa535d7e70",
   "metadata": {},
   "source": [
    "1. Cosine Similarity:\n",
    "\n",
    "Concept: Cosine similarity measures the angle between two vectors in a vector space. If the vectors are perfectly aligned (angle of 0 degrees), their cosine similarity is 1, indicating a high degree of similarity. If they are perpendicular (angle of 90 degrees), their cosine similarity is 0, indicating no similarity.\n",
    "Implementation:\n",
    "Each user's preference vector and each company's feature vector are represented as vectors in a 10-dimensional space.\n",
    "The cosine similarity between a user's preference vector and a company's feature vector is calculated using the dot product of the two vectors divided by the product of their magnitudes.\n",
    "The top 3 companies with the highest cosine similarity to the user's preferences are recommended.\n",
    "\n",
    "\n",
    "2. TF-IDF (Term Frequency-Inverse Document Frequency):\n",
    "\n",
    "Concept: TF-IDF is a weighting scheme used in information retrieval to assign weights to terms in a document based on their frequency within the document and their importance across the entire corpus.\n",
    "Implementation:\n",
    "The user preferences and company features are converted into text representations (e.g., by joining the feature values with spaces).\n",
    "A TF-IDF vectorizer is used to calculate the TF-IDF weights for each term in the user preferences and company features.\n",
    "The cosine similarity between the TF-IDF vectors of a user's preferences and a company's features is calculated.\n",
    "The top 3 companies with the highest cosine similarity based on TF-IDF weights are recommended.\n",
    "\n",
    "\n",
    "Key Differences:\n",
    "\n",
    "Data Representation: Cosine similarity directly uses numerical feature vectors, while TF-IDF requires converting the features into text representations.\n",
    "Weighting: Cosine similarity treats all features equally, while TF-IDF assigns weights to terms based on their frequency and importance.\n",
    "Focus: Cosine similarity focuses on overall similarity between vectors, while TF-IDF emphasizes the importance of specific terms.\n",
    "When to Use Which Approach:\n",
    "\n",
    "Numerical Data: If your data consists of numerical features, cosine similarity is generally suitable, as it directly works with numerical values.\n",
    "Textual Data: If your data is in text format (e.g., job descriptions, user reviews), TF-IDF is more appropriate, as it can capture the semantic meaning of the text.\n",
    "Feature Importance: If you want to emphasize the importance of specific features or terms, TF-IDF can be beneficial, as it assigns higher weights to more important terms.\n",
    "Data Sparsity: If your data is sparse (with many zero values), TF-IDF can help handle sparsity by downweighting frequently occurring terms.\n",
    "Choosing the Right Approach:\n",
    "\n",
    "The best approach depends on the nature of your data, the specific requirements of your recommendation system, and the desired trade-offs between accuracy, interpretability, and computational efficiency. Experimentation with both approaches can help you determine the most suitable one for your particular use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2cd32-9d42-4b20-bc22-ab837b5e2ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
