{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea61a15-7c25-42d3-b83c-d5065f0da289",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The leaves on the tree have fallen.\",\n",
    "    \"She is running towards the finish line.\",\n",
    "    \"The cars are parked in the parking lot.\",\n",
    "    \"A quick brown fox jumps over the lazy dog.\",\n",
    "    \"He was walking in the park and saw a beautiful bird.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71fe644f-9f04-428b-8434-8102eae4468f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The leaves on the tree have fallen.\n",
      "Stemmed: the leav on the tree have fallen .\n",
      "\n",
      "Original: She is running towards the finish line.\n",
      "Stemmed: she is run toward the finish line .\n",
      "\n",
      "Original: The cars are parked in the parking lot.\n",
      "Stemmed: the car are park in the park lot .\n",
      "\n",
      "Original: A quick brown fox jumps over the lazy dog.\n",
      "Stemmed: a quick brown fox jump over the lazi dog .\n",
      "\n",
      "Original: He was walking in the park and saw a beautiful bird.\n",
      "Stemmed: he wa walk in the park and saw a beauti bird .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Deep\n",
      "[nltk_data]     Salunkhe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for sentence in corpus:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    stemmed_words = [ps.stem(word) for word in words]\n",
    "    print(\"Original:\", sentence)\n",
    "    print(\"Stemmed:\", \" \".join(stemmed_words))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1802ff58-1e5d-444d-9145-3a99f24d357b",
   "metadata": {},
   "source": [
    "The PorterStemmer is a common stemming algorithm. It works by chopping off the ends of words in the hope of achieving the correct base form. However, it can sometimes produce non-words and might be too aggressive in some cases.\n",
    "Example: The word running would be stemmed to run, but the word better would be stemmed to bett."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a73c54e5-9d27-45f8-a344-2c983518d1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Deep\n",
      "[nltk_data]     Salunkhe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Deep\n",
      "[nltk_data]     Salunkhe\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The leaves on the tree have fallen.\n",
      "Lemmatized: The leave on the tree have fall .\n",
      "\n",
      "Original: She is running towards the finish line.\n",
      "Lemmatized: She be run towards the finish line .\n",
      "\n",
      "Original: The cars are parked in the parking lot.\n",
      "Lemmatized: The cars be park in the park lot .\n",
      "\n",
      "Original: A quick brown fox jumps over the lazy dog.\n",
      "Lemmatized: A quick brown fox jump over the lazy dog .\n",
      "\n",
      "Original: He was walking in the park and saw a beautiful bird.\n",
      "Lemmatized: He be walk in the park and saw a beautiful bird .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for sentence in corpus:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words]\n",
    "    print(\"Original:\", sentence)\n",
    "    print(\"Lemmatized:\", \" \".join(lemmatized_words))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ddc84-eef8-400b-8214-fafb998cc51a",
   "metadata": {},
   "source": [
    "The WordNetLemmatizer uses WordNet's built-in lexical database to find the base (lemma) of a word, considering its Part of Speech (POS).\n",
    "This approach is generally more accurate than stemming since it results in actual words, but it requires the correct POS tagging.\n",
    "Example: running is lemmatized to run (verb form), and better would stay better as it's already in its lemma form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e8995f-4e18-4991-a779-9fcde5305823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Original: The leaves on the tree have fallen.\n",
      "Lemmatized (spaCy): the leave on the tree have fall .\n",
      "\n",
      "Original: She is running towards the finish line.\n",
      "Lemmatized (spaCy): she be run towards the finish line .\n",
      "\n",
      "Original: The cars are parked in the parking lot.\n",
      "Lemmatized (spaCy): the car be park in the parking lot .\n",
      "\n",
      "Original: A quick brown fox jumps over the lazy dog.\n",
      "Lemmatized (spaCy): a quick brown fox jump over the lazy dog .\n",
      "\n",
      "Original: He was walking in the park and saw a beautiful bird.\n",
      "Lemmatized (spaCy): he be walk in the park and see a beautiful bird .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "# Load English tokenizer, POS tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for sentence in corpus:\n",
    "    doc = nlp(sentence)\n",
    "    lemmatized_words = [token.lemma_ for token in doc]\n",
    "    print(\"Original:\", sentence)\n",
    "    print(\"Lemmatized (spaCy):\", \" \".join(lemmatized_words))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c7739a9-597b-4ab7-bbc8-69299b081212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.6-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.5-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\anaconda\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\anaconda\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Using cached blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\anaconda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\anaconda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.19.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\anaconda\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.0-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in d:\\anaconda\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\anaconda\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.7.6-cp311-cp311-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.6/12.1 MB 17.8 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.7/12.1 MB 9.5 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.9/12.1 MB 7.0 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/12.1 MB 7.5 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/12.1 MB 7.5 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.0/12.1 MB 3.9 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.6/12.1 MB 5.3 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.7/12.1 MB 5.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.9/12.1 MB 4.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/12.1 MB 4.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/12.1 MB 4.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/12.1 MB 4.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.7/12.1 MB 4.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.8/12.1 MB 4.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.0/12.1 MB 4.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.2/12.1 MB 4.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.3/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.5/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.8/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.0/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.0/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.4/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.5/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.7/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.8/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.0/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.2/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.3/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.5/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.6/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.8/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.0/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.2/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.4/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.5/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.7/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.9/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.0/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.2/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.4/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.6/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.8/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 8.0/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 8.2/12.1 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.3/12.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.5/12.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.7/12.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.9/12.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.0/12.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.2/12.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.3/12.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.1 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.7/12.1 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.0/12.1 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.1/12.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.3/12.1 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.5/12.1 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.6/12.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.8/12.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.0/12.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.2/12.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.4/12.1 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.6/12.1 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/12.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 3.8 MB/s eta 0:00:00\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 182.0/182.0 kB 5.5 MB/s eta 0:00:00\n",
      "Using cached murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "   ---------------------------------------- 0.0/479.7 kB ? eta -:--:--\n",
      "   -------------------------------------- 479.7/479.7 kB 10.0 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.5-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.6/1.5 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.5 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 5.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 4.3 MB/s eta 0:00:00\n",
      "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.3/47.3 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.3/50.3 kB 2.5 MB/s eta 0:00:00\n",
      "Using cached blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "Downloading cloudpathlib-0.19.0-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.4/49.4 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.6/5.4 MB 17.5 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.7/5.4 MB 7.8 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.9/5.4 MB 7.2 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.1/5.4 MB 6.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.3/5.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.5/5.4 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.6/5.4 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.8/5.4 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.9/5.4 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.2/5.4 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.3/5.4 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.5/5.4 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 2.6/5.4 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.8/5.4 MB 4.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.0/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.5/5.4 MB 4.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.4 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.8/5.4 MB 4.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.0/5.4 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.2/5.4 MB 4.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.4/5.4 MB 4.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.5/5.4 MB 4.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 4.9/5.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.1/5.4 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.4/5.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.4/5.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 4.0 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.0-cp311-cp311-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 152.6/152.6 kB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.19.0 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 murmurhash-1.0.10 preshed-3.0.9 shellingham-1.5.4 spacy-3.7.6 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.12.5 wasabi-1.1.3 weasel-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6632e240-9e7e-46f6-9fad-4032d2b4ee97",
   "metadata": {},
   "source": [
    "spaCy provides lemmatization out of the box using its pre-trained models. It's more sophisticated than NLTK’s WordNetLemmatizer, as it considers the context of the word within the sentence.\n",
    "Example: The phrase was walking would correctly be lemmatized to be walk (where was becomes be and walking becomes walk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e9e03c4-ac16-4448-acac-60a5521eaff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The leaves on the tree have fallen.\n",
      "Stemmed (spaCy+Porter): the leav on the tree have fallen .\n",
      "\n",
      "Original: She is running towards the finish line.\n",
      "Stemmed (spaCy+Porter): she is run toward the finish line .\n",
      "\n",
      "Original: The cars are parked in the parking lot.\n",
      "Stemmed (spaCy+Porter): the car are park in the park lot .\n",
      "\n",
      "Original: A quick brown fox jumps over the lazy dog.\n",
      "Stemmed (spaCy+Porter): a quick brown fox jump over the lazi dog .\n",
      "\n",
      "Original: He was walking in the park and saw a beautiful bird.\n",
      "Stemmed (spaCy+Porter): he wa walk in the park and saw a beauti bird .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combining spaCy with NLTK for stemming\n",
    "for sentence in corpus:\n",
    "    doc = nlp(sentence)\n",
    "    stemmed_words = [ps.stem(token.text) for token in doc]\n",
    "    print(\"Original:\", sentence)\n",
    "    print(\"Stemmed (spaCy+Porter):\", \" \".join(stemmed_words))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b096d5-b93c-4078-86dd-a80b34c8bc61",
   "metadata": {},
   "source": [
    "NLTK’s PorterStemmer is a rule-based stemmer that sometimes produces results that are not real words (e.g., running -> run, better -> bett). It's faster but less accurate than lemmatization.\n",
    "\n",
    "NLTK’s WordNetLemmatizer uses WordNet's lexical database for more accurate lemmatization. However, it requires correct POS tagging to work well, which can be a limitation if you're processing large amounts of text without POS tagging.\n",
    "\n",
    "spaCy’s Lemmatizer is more advanced and context-aware, making it generally more accurate than NLTK’s WordNetLemmatizer. It’s better suited for tasks requiring higher accuracy in natural language processing.\n",
    "\n",
    "spaCy + NLTK’s PorterStemmer allows for stemming within the spaCy framework, though it's not as common as using lemmatization in spaCy.\n",
    "\n",
    "For most modern NLP applications, lemmatization (especially using spaCy) is preferred over stemming due to its accuracy and context-awareness. However, stemming might still be useful in specific scenarios where speed is crucial and slight inaccuracies can be tolerated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ebf576-6275-4dfd-82f4-ec5a60ab0496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
